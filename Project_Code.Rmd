## Final Project (Submitted by Hardik Guota hg8675 and Khushboo Thakkar kt24992)

library(arules)
library(arulesViz)
library(grid)
library(dplyr)
library(methods)


## Getting countries elevation data for each year
elev = read.csv("E:/Project_Scott/ac_elev.csv")
a = matrix(0,162,3)
b = matrix(0,162,3)
a[,1] = as.character(elev[,1]) ## Country's name
a[,2] = 2001
a[,3] = elev[,2] ## Country's elevation

for (i in 1:17){
  b[,1] = as.character(elev[,1])
  b[,2] = 2001+i     ##Country's year wise distribution
  b[,3] = elev[,2]
  a <- rbind(a, b)  ## Appending the last year compiled data with this year
}

write.csv(a, "E:/Project_Scott/elev_final.csv")

## % Share of population in rural area

rur = read.csv("E:/Project_Scott/rural_pop.csv")

rur_n = rur[,c(1,2,46:63)] ## subsetting the data for relevant columns

rur_n1 = rur_n[c(4:267),] ## Subsetting for relevant rows

a = matrix(0,264,4)  ## generating null matrices
b = matrix(0,264,4)
a[,1] = as.character(rur_n1[,1])
a[,2] = as.character(rur_n1[,2])
a[,3] = 2001
a[,4] = rur_n1[,3]

for (i in 1:17){
  b[,1] = as.character(rur_n1[,1])
  b[,2] = as.character(rur_n1[,2]) # Country's code
  b[,3] = 2001+i
  b[,4] = rur_n1[,3+i] 
  a <- rbind(a, b) ## appending the data with last year data
}

write.csv(a, "E:/Project_Scott/rural_final.csv")

## Coastal length for each country

cl = read.csv("E:/Project_Scott/coast_len.csv")

cl_n = cl[,c(1,2)]  ## Subsetting for important columns

a = matrix(0,193,3)
b = matrix(0,193,3)
a[,1] = as.character(cl_n[,1])
a[,2] = 2001
a[,3] = as.character(cl_n[,2])

for (i in 1:17){
  b[,1] = as.character(cl_n[,1])
  b[,2] = 2001+i
  b[,3] = as.character(cl_n[,2])
  a <- rbind(a, b)  ## appending the previous year's data with this (assuming the coastal length doesn't change)
}

write.csv(a, "E:/Project_Scott/coast_len_final.csv")

## percentage share of population under poverty

pov = read.csv("E:/Project_Scott/poverty.csv")

pov_n = subset(pov, X == 'Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)', c(1,2,46:63) ) ## taking relevant columns
a = matrix(0,264,4)
b = matrix(0,264,4)
a[,1] = as.character(pov_n[,1])
a[,2] = as.character(pov_n[,2])
a[,3] = 2001
a[,4] = pov_n[,3]

for (i in 1:17){
  b[,1] = as.character(pov_n[,1])
  b[,2] = as.character(pov_n[,2])
  b[,3] = 2001+i
  b[,4] = pov_n[,3+i]
  a <- rbind(a, b) ## appending the previous year's data with this year's
}

write.csv(a, "E:/Project_Scott/poverty_final.csv")

## Total land area of the country

land = read.csv("E:/Project_Scott/landarea.csv")

land_n = land[,c(1,2,46:63)] ## subsetting relevant columns 

land1 = land_n[c(4:267),] ## subsetting relevant rows

a = matrix(0,264,4)
b = matrix(0,264,4)
a[,1] = as.character(land1[,1])
a[,2] = as.character(land1[,2])
a[,3] = 2001
a[,4] = rur_n1[,3]

for (i in 1:17){
  b[,1] = as.character(land1[,1])
  b[,2] = as.character(land1[,2])
  b[,3] = 2001+i
  b[,4] = land1[,3+i]
  a <- rbind(a, b) ## appending all the relevant data
}

write.csv(a, "E:/Project_Scott/land_final.csv")

## share of renewable energy generated by country

renew = read.csv("E:/Project_Scott/re_energy_cons.csv")

renew_1 = renew[,c(1,2,46:63)] ## subsetting relevant columns from WB data

renew1 = renew_1[c(4:267),] ## subsetting relevant rows

a = matrix(0,264,4)
b = matrix(0,264,4)

a[,1] = as.character(renew1[,1])
a[,2] = as.character(renew1[,2])
a[,3] = 2001
a[,4] = renew1[,3]

for (i in 1:17){
  b[,1] = as.character(renew1[,1])
  b[,2] = as.character(renew1[,2])
  b[,3] = 2001+i
  b[,4] = renew1[,3+i]
  a <- rbind(a, b) ## appending this year's data with previous one
}


write.csv(a, "E:/Project_Scott/renewable_final.csv")

## People addicted to smoking by Country

smoke = read.csv("E:/Project_Scott/daily-smoking-prevalence-bounds.csv")


smoke_n = smoke[,c(1,2,3,6)] ## subsetting relevant columns

write.csv(smoke_n, "E:/Project_Scott/smoke_final.csv")

# Similarly we try to extract the variable of interest corresponding to each country and for year from the raw data files downloaded from the website of World Bank.

library(arules)
library(arulesViz)
library(grid)
library(dplyr)
library(methods)
## File
pov = read.csv("E:/Project_Scott/poverty.csv")

pov_n = subset(pov, X == 'Poverty headcount ratio at $1.90 a day (2011 PPP) (% of population)', c(1,2,46:63) )
a = matrix(0,264,4)
b = matrix(0,264,4)
a[,1] = as.character(pov_n[,1])
a[,2] = as.character(pov_n[,2])
a[,3] = 2001
a[,4] = pov_n[,3]

for (i in 1:17){
  b[,1] = as.character(pov_n[,1])
  b[,2] = as.character(pov_n[,2])
  b[,3] = 2001+i
  b[,4] = pov_n[,3+i]
  a <- rbind(a, b)
}

write.csv(a, "poverty_final.csv")

# We repeat the above code for the raw data on all the variables extracted from the world bank website and create and save a separate csv file for each variable. These include land under agriculture as a percentage of total land area, land under forest as a percentage of total land area, population density, poverty, renewable energy consumption as a percentage of total energy consumed, precipitation, share of slums, rural population as a percentage of total population, Research & development, total land area, GDP, GDP per capita, API (Air Pollution Index) and value added by manufacturing as a percentage of GDP.

# Having done this, we use all the saved CSV files and compile them into one CSV file (raw data files can be provided if required but are not attached) using Vlookup in Excel. We then filtered the countries in ascending order in order to get data in the desired long form format for panel datasets.

## Data Cleaning

library(arules)
library(arulesViz)
library(grid)
library(dplyr)
library(methods)
library(dplyr)
## File
dat = read.csv("E:/Project_Scott/New csv/final_worldbank.csv")
dat1 = select(dat, -12,-17,-14)
dat2 = subset(dat1, costlen != '#N/A')
dat3 = subset(dat2, year!='2017')
dat4 = subset(dat3, year!= '2018')
dat4$costlen = gsub("[a-zA-Z ]", "", dat4$costlen)
dat4$costlen = as.numeric(as.factor(dat4$costlen))

write.csv(dat4, "E:/Project_Scott/project_final.csv")

library(arules)
library(arulesViz)
library(grid)
library(dplyr)
library(methods)
library(dplyr)
## File
dat = read.csv("E:/Project_Scott/New csv/data.csv")

dat <- na.omit(dat)

write.csv(dat, "E:/Project_Scott/New csv/final_data.csv")

## In order to balance the panel, a few countries like ike Bermuda, Chad, Eritrea, Somalia, Mongolia, that did not have data for all the years from 2010 to 2016, were eliminated and this operation was carried out using pivot table in excel. The final data file has been sent as an attachment in the mail.

## Data Analysis

## Graphical Analysis

library(R.utils)
library(arules)
library(arulesViz)
library(grid)
library(dplyr)
library(methods)
library(tidyverse)
library(mosaic)
library(ggplot2)
library(gridExtra)
library(FactoMineR)
library(factoextra)
library(plm)
library(gganimate)
library(gapminder)
library(gifski)
library(magick)
library(plm)
library(xtable)
library(foreign)
library(car)
library(panelView)
library(broom)
library(knitr)
library(lmtest)

ap = read.csv("~/Downloads/ECO395M-master-2/data/final_data.csv")
summary(ap)
nrow(ap)

# Defining a new variable that classifies countries based on GDP per capita

ap$classification[ap$gdppc>50000] <- ">50k"
ap$classification[35000<ap$gdppc & ap$gdppc<=50000] <- "35K-50K"
ap$classification[20000<ap$gdppc & ap$gdppc<=35000] <- "20K-35K"
ap$classification[10000<ap$gdppc & ap$gdppc<=20000] <- "10K-20K"
ap$classification[5000<ap$gdppc & ap$gdppc<=10000] <- "5K-10K"
ap$classification[2000<ap$gdppc & ap$gdppc<=5000] <- "2K-5K"
ap$classification[0<ap$gdppc & ap$gdppc<=2000] <- "<2K"

view(ap)

options(scipen=999)

## Time varying plots

theme_set(theme_bw()) 

p <- ggplot(ap, aes(x = manu, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Manufacturing as % of GDP", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p

p01 = p + transition_time(year) + labs(title = "Year: {frame_time}")
p01

anim_save("~/Downloads/ECO395M-master-2/data/p01.gif", p01)

theme_set(theme_bw()) 

p1 <- ggplot(ap, aes(x = agriland, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Agricultural land as % of land area", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p1

p02 = p1 + transition_time(year) + labs(title = "Year: {frame_time}")
p02

anim_save("~/Downloads/ECO395M-master-2/data/p02.gif", p02)

theme_set(theme_bw()) 

p2 <- ggplot(ap, aes(x = forland, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Forest land as % of land area", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p2

p03 = p2 + transition_time(year) + labs(title = "Year: {frame_time}")
p03

anim_save("~/Downloads/ECO395M-master-2/data/p03.gif", p03)

theme_set(theme_bw()) 

p3 <- ggplot(ap, aes(x = gdppc, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "GDP per capita", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p3

p04 = p3 + transition_time(year) + labs(title = "Year: {frame_time}")
p04

anim_save("~/Downloads/ECO395M-master-2/data/p04.gif", p04)

p4 <- ggplot(ap, aes(x = ruralpop, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Rural population (% of total population)", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p4

p05 = p4 + transition_time(year) + labs(title = "Year: {frame_time}")
p05

anim_save("~/Downloads/ECO395M-master-2/data/p05.gif", p05)

theme_set(theme_bw()) 

p5 <- ggplot(ap, aes(x = rencons, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Renewable energy consumed", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p5

p06 = p5 + transition_time(year) + labs(title = "Year: {frame_time}")
p06

anim_save("~/Downloads/ECO395M-master-2/data/p06.gif", p06)

theme_set(theme_bw()) 

p6 <- ggplot(ap, aes(x = popden, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Population density", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) + xlim(c(0, 2000))
p6

p07 = p6 + transition_time(year) + labs(title = "Year: {frame_time}")
p07

anim_save("~/Downloads/ECO395M-master-2/data/p07.gif", p07)

p7 <- ggplot(ap, aes(x = smoking, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "Smoking (% of adults)", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p7

p08 = p7 + transition_time(year) + labs(title = "Year: {frame_time}")
p08

anim_save("~/Downloads/ECO395M-master-2/data/p08.gif", p08)

ap$rencons1 = as.numeric(ap$rencons)
summary(ap)

p8 <- ggplot(ap, aes(x = rencons1, y=api, size = popden, colour = classification)) + geom_point() + labs(x = "% of Renewable energy consumed", y = "Air pollution") + guides(color=guide_legend("Income Per Capita"), size=guide_legend("Population Density")) 
p8

p09 = p8 + transition_time(year) + labs(title = "Year: {frame_time}")
p09

anim_save("~/Downloads/ECO395M-master-2/data/p09.gif", p09)

## Panel Data Analysis

# Setting the data as panel data

# Note: Our data was compiled in the long format  and therefore, each row is one time point per subject. So each subject (country) will have data in multiple rows. Any variables that don’t change across time will have the same value in all the rows.

panelap <- pdata.frame(ap, index=c("country", "year"))

# Checking the dimensions of the panel data
pdim(panelap)

# Panel data plot of the Air Pollution Index over time across countries
panelView(api ~ 1, data = ap, index = c("country","year"), type = "outcome", main = "Air Pollution Index over the years", ylim = c(0,100), xlab = "Year", ylab = "API")

# The fixed effects estimator assumes that the variables that are fixed over time (constant for each panel) may be correlated with the time varying variables of the model. Use of the random effects model implies the additional orthogonality conditions—that the time-invariant regressors are not correlated with the time-varying regressors—and yields inference about the underlying population that is not conditional on the fixed effects in our sample.

# We use the Hausman test to decide whether a fixed effects model or a random effects model would be more appropriate. 

# Implementing a fixed effects model
library(plm)
panelap$rencons1 = as.numeric(panelap$rencons)

fixedap = plm(api ~ agriland + forland + gdppc + popden + manu + rencons1 + ruralpop + landarea + precipitation + smoking + costlen + elev, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap)

kable(tidy(fixedap), digits=3, caption="Fixed Effects-All Variables")

# Implementing a random effects model

randomap = plm(api ~ agriland + forland + gdppc + popden + manu + rencons1 + ruralpop + landarea + precipitation + smoking + costlen + elev, data = panelap, index = c("country", "year"), model = "random")
summary(randomap)

kable(tidy(randomap), digits=3)
# The Hausman test: The null hypothesis under this test is that the null hypothesis is that the preferred model is random effects versus the alternative that the fixed effects model is better

phtest(fixedap, randomap)

# The p-value is 0.02 which is less than 0.05 and therefore, we reject the null hypothesis and resort to using the fixed-effects model going forward.

# Model 1
fixedap1 = plm(api ~ agriland + forland + gdppc + popden + manu + rencons1 + ruralpop, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap1)
r.rmse1 <- sqrt(mean(residuals(fixedap1)^2))
r.rmse1

# Model 2
fixedap2 = plm(api ~ agriland + forland + gdppc + manu + ruralpop, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap2)
r.rmse2 <- sqrt(mean(residuals(fixedap2)^2))
r.rmse2

# Model 3
fixedap3 = plm(api ~ agriland + gdppc + manu + ruralpop, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap3)
r.rmse3 <- sqrt(mean(residuals(fixedap3)^2))
r.rmse3

# Model 4
fixedap4 = plm(api ~ agriland + gdp + manu + ruralpop, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap4)
r.rmse4 <- sqrt(mean(residuals(fixedap4)^2))
r.rmse4

# Model 5
fixedap5 = plm(api ~  manu + gdppc:manu + ruralpop, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap5)
r.rmse5 <- sqrt(mean(residuals(fixedap5)^2))
r.rmse5

# Model 6
fixedap6 = plm(api ~  manu + gdppc + forland + popden + forland:popden, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap6)
r.rmse6 <- sqrt(mean(residuals(fixedap6)^2))
r.rmse6

# Model 7
fixedap7 = plm(api ~  poly(manu, 2) + gdp + ruralpop, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap7)
r.rmse7 <- sqrt(mean(residuals(fixedap7)^2))
r.rmse7

# Final model
fixedap8 = plm(api ~  poly(manu, 2) + poly(gdppc,2) + agriland, data = panelap, index = c("country", "year"), model = "within")
summary(fixedap8)
r.rmse8 <- sqrt(mean(residuals(fixedap8)^2))
r.rmse8

# Cluster robust standard errors for the final model
robust = coeftest(fixedap8, vcov=vcovHC(fixedap8, type="sss", cluster="group"))
kable(tidy(robust), digits=3)

# Same model with GDP pc equal to or below 50K (Begin by creating a subset)
subset1 = subset(panelap,  panelap$gdppc <= 50000)
nrow(subset1)
fixedap9 = plm(api ~  poly(manu, 2) + poly(gdppc,2) + agriland, data = subset1, index = c("country", "year"), model = "within")
summary(fixedap8)
r.rmse9 <- sqrt(mean(residuals(fixedap9)^2))
r.rmse9

# Cluster robust standard errors for the final model with GDP pc <= 50K
robust = coeftest(fixedap9, vcov=vcovHC(fixedap9, type="sss", cluster="group"))
kable(tidy(robust), digits=3)

# Same inference as for the entire sample

# Note: R squared is not very informative in this case. In panel data analysis, reliance should be placed more on individual significance and overall significance of the model instead of R squared or adjusted R squared. Generally, R squared is low in cross sectional data as compared to time series data. In panel data due to heterogeneity of cross sections, it is not too high. Given that our data is more cross section dominant, R squared will be lower as compared to the case when panel data is more time dominant. 


